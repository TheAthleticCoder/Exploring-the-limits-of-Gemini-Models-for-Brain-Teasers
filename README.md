# iREL at SemEval-2024 Task 9: Exploring the limits of Gemini Models for Brain Teasers

**This repo contain the code for our approach towards SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense.** The BRAINTEASER task comprises multiple-choice Question Answering, designed to evaluate the models’ lateral thinking capabilities. It consists of Sentence Puzzle and Word Puzzle subtasks that require models to defy default common-sense associations and exhibit unconventional thinking. We propose a unique strategy aimed at improving the performance of pre-trained language models, particularly the Gemini 1.0 Pro Model, in both subtasks. We employ static and dynamic few-shot prompting techniques and introduce a model-generated reasoning strategy that utilizes the LLM’s reasoning capabilities to improve its performance. Our approach demonstrated significant improvements, showing that it performed better than the baseline models by a significant margin but fell short of performing as well as the human annotators, thus highlighting the efficacy of the proposed strategies.

---

## Semeval Task Description Paper

```
@inproceedings{jiang-semeval-2024-brainteaser, title = "SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense", author = "Jiang, Yifan and Ilievski, Filip and Ma, Kaixin", booktitle = "Proceedings of the 18th International Workshop on Semantic Evaluation", year = "2024", publisher = "Association for Computational Linguistics"}
```

---
